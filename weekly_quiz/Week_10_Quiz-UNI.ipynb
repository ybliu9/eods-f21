{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Quiz\n",
    "\n",
    "## Name - UNI\n",
    "\n",
    "### Due Sun. Nov 28th 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this quiz, we're going to load documents from 2 categories (space, cars) in the 20newsgroups dataset.\n",
    "\n",
    "The goal is to train a classifier that classifies documents into these 2 categories based on a term frequency representation of the documents.\n",
    "\n",
    "We will then calculate mean cross-validaion accuracy of a RandomForestClassifier using this transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fetch_20newsgroups from sklearn.datasets\n",
    "____\n",
    "\n",
    "# Load the dataset using fetch_20newsgroups().\n",
    "#   Only fetch the two categories of interest using categories=['sci.space','rec.autos']\n",
    "# Store in the result into newsgroups\n",
    "____\n",
    "\n",
    "# Store the newsgroups.data as docs, newsgroups.target as y and newsgroups.target_names as y_names\n",
    "____\n",
    "\n",
    "# Print the number of observations by printing the length of docs\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the text of the first document in docs.\n",
    "# Note: try printing both with and without the print() statement\n",
    "#  with the print statement, linebreaks are parsed,\n",
    "#  without, linebreaks are printed as excape characters\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the target value of the first document in y.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the target_name of the first document using y_names and y.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CountVectorizer to Convert To TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer from sklearn\n",
    "____\n",
    "\n",
    "# Initialize a CountVectorizer object. It should\n",
    "#   lowercase all text, \n",
    "#   include both unigrams and bigrams\n",
    "#   exclude terms that occur in fewer than 10 documents\n",
    "#   exclude terms that occur in more than 95% of documents\n",
    "# Store as cvect\n",
    "____\n",
    "\n",
    "# Fit cvect on docs and transform docs into their term frequency representation.\n",
    "# Store as X_tf\n",
    "____\n",
    "\n",
    "# Print the shape of X_tf. \n",
    "# The number of rows should match the number of documents \n",
    "#    and the number of columns should be in the thousands\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the last 5 terms in the learned vocabulary in cvect\n",
    "#   using .get_feature_names() which returns a list of terms corresponding\n",
    "#   to the order of the columns in X_tf\n",
    "# They should all be related to zoos\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stopwords learned by cvect are stored as a set in cvect.stop_words_\n",
    "# We'd like to print out a small subset of these terms.\n",
    "# One way to get a subset of a set is to treat it as a list.\n",
    "# First, convert the stop_words_ set to a list.\n",
    "# Store as stop_words_list\n",
    "____\n",
    "\n",
    "# Print out the first 5 elements in stop_words_list.\n",
    "# Note that, since a set is unordered, \n",
    "#    there is no meaning to the ordering of these terms and they may vary over runs.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean CV Accuracy Using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross_val_score from sklearn\n",
    "____\n",
    "\n",
    "# Import RandomForestClassifier from sklearn\n",
    "____\n",
    "\n",
    "# Get a set of 5-fold CV scores using\n",
    "#    a RandomForestClassifier with 50 trees and n_jobs=-1\n",
    "#    and the full dataset X_tf and y\n",
    "# Store as cv_scores\n",
    "____\n",
    "\n",
    "# Print the mean of these cv_scores. The mean accuracy should be above .9\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Find Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer stores the feature names (terms in the vocabulary) in two ways:\n",
    "#  1. as a dictionary of term:colum_index pairs, accessed via cvect.vocabulary_\n",
    "#  2. as a list of terms, in column index order, accessed via cvect.get_feature_names()\n",
    "#\n",
    "# We can get the indices of the most important features by retraining a RandomForestClassifier on X_tf,y\n",
    "#  and accessing .feature_importances_\n",
    "#\n",
    "# Using some combination of the above data-structures, \n",
    "#  print out the top 10 terms in the vocabulary\n",
    "#  ranked by the feature importances learned by a RandomForestClassifier with 50 trees\n",
    "# \n",
    "# The terms you find will likely not be surprising given the document categories.\n",
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f21",
   "language": "python",
   "name": "eods-f21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
