{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Elements Of Data Science - F2021</center>\n",
    "# <center>Week 10: NLP, Sentiment Analysis and Topic Modeling<center>\n",
    "### <center>11/22/2021</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TODOs\n",
    "\n",
    "- Readings:\n",
    "  - [PDSH 5.11 k-Means](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n",
    "  - [Recommended] [PML Chapter 11: Working with Unlabeled Data - Clustering Analysis](https://ezproxy.cul.columbia.edu/login?qurl=https%3a%2f%2fsearch.ebscohost.com%2flogin.aspx%3fdirect%3dtrue%26db%3de025xna%26AN%3d1606531%26site%3dehost-live%26scope%3dsite%26ebv%3DEB%26ppid%3Dpp_347) except for last section on DBScan\n",
    "  - [Optional] [Data Science From Scratch Chap 22: Recommender Systems](https://ezproxy.cul.columbia.edu/login?qurl=https%3a%2f%2fsearch.ebscohost.com%2flogin.aspx%3fdirect%3dtrue%26db%3dnlebk%26AN%3d979529%26site%3dehost-live%26scope%3dsite%26ebv%3DEB%26ppid%3Dpp_267)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- HW3, Due **Tues Nov 23rd 11:59pm**\n",
    "\n",
    "- Answer and submit Quiz 10, **Sunday Nov 28, 11:59pm ET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today\n",
    "\n",
    "- **Pipelines**\n",
    "- **NLP**\n",
    "- **Sentiment Analysis**\n",
    "- **Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# <center>Questions?</center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn\n",
    "\n",
    "- Pipelines are wrappers used to string together transformers and estimators\n",
    "- sequentially apply a series of transforms, eg, `.fit_transform()` and `.transform()`\n",
    "- followed by a prediction, eg. `.fit()` and `.predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align=\"center\"><img src=\"images/pipelines.png\" width=\"800px\"></div>\n",
    "\n",
    "<font size=6>From PML</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Binary Classification With All Numeric Features Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.094e+01, 1.859e+01, 7.039e+01, 3.700e+02, 1.004e-01, 7.460e-02,\n",
       "        4.944e-02, 2.932e-02, 1.486e-01, 6.615e-02, 3.796e-01, 1.743e+00,\n",
       "        3.018e+00, 2.578e+01, 9.519e-03, 2.134e-02, 1.990e-02, 1.155e-02,\n",
       "        2.079e-02, 2.701e-03, 1.240e+01, 2.558e+01, 8.276e+01, 4.724e+02,\n",
       "        1.363e-01, 1.644e-01, 1.412e-01, 7.887e-02, 2.251e-01, 7.732e-02]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example from PML - scaling > feature extraction > classification\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "bc = load_breast_cancer()\n",
    "X_bc,y_bc = bc['data'],bc['target']\n",
    "X_bc_train,X_bc_test,y_bc_train,y_bc_test = train_test_split(X_bc,\n",
    "                                                             y_bc,\n",
    "                                                             test_size=0.2,\n",
    "                                                             stratify=y_bc,\n",
    "                                                             random_state=123)\n",
    "\n",
    "# all real valued features\n",
    "X_bc_train[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set accuracy: 0.956\n",
      "test set accuracy : 0.956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Pipeline: list of (name,object) pairs\n",
    "pipe1 = Pipeline([('scale',StandardScaler()),                  # scale\n",
    "                 ('pca',PCA(n_components=2)),                  # reduce dimensions\n",
    "                 ('lr',LogisticRegression(solver='saga',\n",
    "                                          max_iter=1000,\n",
    "                                          random_state=123)),  # classifier\n",
    "                ])\n",
    "\n",
    "pipe1.fit(X_bc_train,y_bc_train)\n",
    "\n",
    "print(f'train set accuracy: {pipe1.score(X_bc_train,y_bc_train):0.3f}')\n",
    "print(f'test set accuracy : {pipe1.score(X_bc_test,y_bc_test):0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.00439115,  1.11969368]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access pipeline components by name like a dictionary\n",
    "pipe1['lr'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21777854, 0.08876361, 0.22663097, 0.22043131, 0.14913361,\n",
       "       0.23954684, 0.25974993, 0.26277752, 0.14518851, 0.06537618,\n",
       "       0.20775303, 0.0074925 , 0.21143104, 0.2018041 , 0.0165253 ,\n",
       "       0.17152404, 0.14891828, 0.18380569, 0.03639995, 0.09860293,\n",
       "       0.22726391, 0.09186544, 0.23623194, 0.22416772, 0.13445762,\n",
       "       0.21075345, 0.22996838, 0.25138607, 0.12409848, 0.13331693])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1['pca'].components_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn: GridSearch with Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- specify grid points using 'step name' + '__' (double-underscore) + 'argument'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 1, 'lr__penalty': 'l1', 'pca__n_components': 20}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# separate step-names and argument-names with double-underscore '__'\n",
    "params = {'pca__n_components':[2,10,20],\n",
    "          'lr__penalty':['none','l1','l2'],\n",
    "          'lr__C':[.01,1,10,100]}\n",
    "\n",
    "gscv = GridSearchCV(pipe1, params, cv=3, n_jobs=-1).fit(X_bc_train,y_bc_train)\n",
    "\n",
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "score = gscv.score(X_bc_test,y_bc_test)\n",
    "print(f'test set accuracy: {score:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scale', StandardScaler()), ('pca', PCA(n_components=20)),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=1, max_iter=1000, penalty='l1',\n",
       "                                    random_state=123, solver='saga'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn with `make_pipeline`\n",
    "\n",
    "- shorthand for Pipeline\n",
    "- step names are lowercase of class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('pca', PCA(n_components=2)),\n",
       "                ('logisticregression', LogisticRegression(random_state=123))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# make_pipeline: arguments in order of how they should be applied\n",
    "pipe2 = make_pipeline(StandardScaler(),                    # center and scale data\n",
    "                     PCA(n_components=2),                 # extract 2 dimensions\n",
    "                     LogisticRegression(random_state=123) # classify using logistic regression\n",
    "                    )\n",
    "pipe2.fit(X_bc_train,y_bc_train) \n",
    "\n",
    "pipe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.0068728 ,  1.12126495]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2['logisticregression'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ColumnTransformer\n",
    "\n",
    "- Transform sets of columns differently as part of a pipeline\n",
    "- For example: makes it possible to transform categorical and numeric differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Binary Classification With Mixed Features, Missing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>sex</th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.0</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>S</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age      fare embarked     sex  pclass  survived\n",
       "0  29.0  211.3375        S  female       1         1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py\n",
    "titanic_url = ('https://raw.githubusercontent.com/amueller/'\n",
    "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
    "df_titanic = pd.read_csv(titanic_url)[['age','fare','embarked','sex','pclass','survived']]\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "df_titanic.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       1046 non-null   float64\n",
      " 1   fare      1308 non-null   float64\n",
      " 2   embarked  1307 non-null   object \n",
      " 3   sex       1309 non-null   object \n",
      " 4   pclass    1309 non-null   int64  \n",
      " 5   survived  1309 non-null   int64  \n",
      "dtypes: float64(2), int64(2), object(2)\n",
      "memory usage: 61.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ColumnTransformer Cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# specify columns subset\n",
    "numeric_features = ['age', 'fare']\n",
    "# specify pipeline to apply to those columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')), # fill missing values with median\n",
    "    ('scaler', StandardScaler())])                 # scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # fill missing value with 'missing'\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])                   # one hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# combine column pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numeric_transformer, numeric_features),\n",
    "                  ('cat', categorical_transformer, categorical_features)\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# add a final prediction step\n",
    "pipe3 = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', LogisticRegression(solver='lbfgs', random_state=42))\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ColumnTransformer Cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set score: 0.784\n",
      "test set score : 0.771\n"
     ]
    }
   ],
   "source": [
    "X_titanic = df_titanic.drop('survived', axis=1)\n",
    "y_titanic = df_titanic['survived']\n",
    "\n",
    "X_titanic_train, X_titanic_test, y_titanic_train, y_titanic_test = train_test_split(X_titanic, \n",
    "                                                                                    y_titanic, \n",
    "                                                                                    test_size=0.2, \n",
    "                                                                                    random_state=42)\n",
    "pipe3.fit(X_titanic_train, y_titanic_train)\n",
    "print(f\"train set score: {pipe3.score(X_titanic_train, y_titanic_train):.3f}\")\n",
    "print(f\"test set score : {pipe3.score(X_titanic_test, y_titanic_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best test set score from grid search: 0.771\n",
      "best parameter settings: {'classifier__C': 100, 'preprocessor__num__imputer__strategy': 'median'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# grid search deep inside the pipeline\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "gs_pipeline = GridSearchCV(pipe3, param_grid, cv=3)\n",
    "gs_pipeline.fit(X_titanic_train, y_titanic_train)\n",
    "print(\"best test set score from grid search: {:.3f}\".format(gs_pipeline.score(X_titanic_test, y_titanic_test)))\n",
    "print(\"best parameter settings: {}\".format(gs_pipeline.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# <center>Questions re Pipelines?</center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing (NLP)\n",
    "<br>\n",
    "\n",
    "- Analyzing and interacting with natural language\n",
    "- Python Libraries\n",
    "  - **sklearn**\n",
    "  - nltk\n",
    "  - **spaCy**\n",
    "  - gensim\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing (NLP)\n",
    "<br>\n",
    "\n",
    "- Many NLP Tasks\n",
    "\n",
    "  - **sentiment analysis**\n",
    "  - **topic modeling**\n",
    "  - entity detection\n",
    "  - machine translation\n",
    "  - natural language generation\n",
    "  - question answering\n",
    "  - relationship extraction\n",
    "  - automatic summarization\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recall: Python Builtin String Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D.S. is fun!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"D.S. is fun!\"\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('d.s. is fun!', 'D.S. IS FUN!')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.lower(),doc.upper()       # change capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['D.S.', 'is', 'fun!'], ['D', 'S', ' is fun!'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.split() , doc.split('.')  # split a string into parts (default is whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab|c|d'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join(['ab','c','d'])      # join items in a list together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D|.|S|.| '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join(doc[:5])             # a string itself is treated like a list of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'  test   '.strip()           # remove whitespace from the beginning and end of a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- and many more, see [https://docs.python.org/3.8/library/string.html](https://docs.python.org/3.8/library/string.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: The Corpus\n",
    "<br>\n",
    "\n",
    "- **corpus:** collection of documents\n",
    "  - books\n",
    "  - articles\n",
    "  - reviews\n",
    "  - tweets\n",
    "  - resumes\n",
    "  - sentences?\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Doc Representation\n",
    "<br>\n",
    "\n",
    "- Documents usually represented as strings\n",
    "  - string: a sequence (list) of unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D.S. is fun!\n",
      "It's  true.\n"
     ]
    }
   ],
   "source": [
    "doc = \"D.S. is fun!\\nIt's  true.\"\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"D|.|S|.| |i|s| |f|u|n|!|\\n|I|t|'|s| | |t|r|u|e|.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Need to split this up into parts (**tokens**)\n",
    "- Good job for **Regular Expressions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aside: Regular Expressions\n",
    "<br>\n",
    "\n",
    "- Strings that define search patterns over text\n",
    "- Useful for finding/replacing/grouping\n",
    "- python `re` library (others available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D.S. is fun!\n",
      "It's  true.\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ' ', '\\n', '  ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Find all of the whitespaces in doc\n",
    "# '\\s+' means \"one or more whitespace characters\"\n",
    "re.findall(r'\\s+',doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aside: Regular Expressions\n",
    "\n",
    "Just some of the special character definitions:\n",
    "    \n",
    "- `.` : any single character except newline (r'.' matches 'x')\n",
    "- `*` : match 0 or more repetitions (r'x*' matches 'x','xx','')\n",
    "- `+` : match 1 or more repetitions (r'x+' matches 'x','xx')\n",
    "- `?` : match 0 or 1 repetitions (r'x?' matches 'x' or '')\n",
    "<br>\n",
    "    \n",
    "- `^` : beginning of string (r'^D' matches 'D.S.')\n",
    "- `$` : end of string (r'fun!$' matches 'DS is fun!'`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aside: Regular Expression Cont.\n",
    "<br>\n",
    "\n",
    "- `[]` : a set of characters (^ as first element = not)\n",
    "- `\\s` : whitespace character (Ex: [ \\t\\n\\r\\f\\v])\n",
    "- `\\S` : non-whitespace character (Ex: [^ \\t\\n\\r\\f\\v])\n",
    "- `\\w` : word character (Ex: [a-zA-Z0-9_])\n",
    "- `\\W` : non-word character\n",
    "- `\\b` : boundary between \\w and \\W\n",
    "- and many more!\n",
    "<br>\n",
    "\n",
    "- See [regex101.com](https://regex101.com) for examples and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aside: Regex Python Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\w*u\\\\w*'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r'\\w*u\\w*' # a string of word characters containing u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fun', 'true']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\w*u\\w*',doc) # return all substrings that match a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"D.S. is XXXX!\\nIt's  XXXX.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\w*u\\w*','XXXX',doc) # substitute all substrings that match a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D.S. is ', \"!\\nIt's  \", '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\w*u\\w*',doc) # split substrings on a pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Tokenization\n",
    "\n",
    "- **tokens:** strings that make up a document ('the', 'cat',...)\n",
    "- **tokenization:** convert a document into tokens\n",
    "- **vocabulary:** set of unique tokens (terms) in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D.S.', 'is', 'fun!', \"It's\", 'true.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split on whitespace\n",
    "re.split(r'\\s+', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'fun', 'It', 'true']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find tokens of length 2+ word characters\n",
    "re.findall(r'\\b\\w\\w+\\b',doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D.S', 'is', 'fun', \"It's\", 'true']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find tokens of length 2+ non-space characters\n",
    "re.findall(r\"\\b\\S\\S+\\b\", doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP:Tokenization\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align=\"center\"><img src=\"images/spacy_tokenization.svg\" width=\"1000px\"></align>\n",
    "\n",
    "<font size=5>From [https://spacy.io/usage/linguistic-features](https://spacy.io/usage/linguistic-features)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Other Preprocessing\n",
    "<br>\n",
    "\n",
    "- lowercase\n",
    "- remove special characters\n",
    "- add `<START>`, `<END>` tags\n",
    "- stemming: cut off beginning or ending of word\n",
    "  - 'studies' becomes 'studi'\n",
    "  - 'studying' becomes 'study'\n",
    "- lemmatization: perform morphological analysis\n",
    "  - 'studies' becomes 'study'\n",
    "  - 'studying' becomes 'study'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Bag of Words\n",
    "    \n",
    "- BOW representation: ignore token order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d.s', 'fun', 'is', \"it's\", 'true']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(re.findall(r'\\b\\S\\S+\\b', doc.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: n-Grams\n",
    "\n",
    "- Unigram: single token\n",
    "- Bigram: combination of two ordered tokens\n",
    "- n-Gram: combination of n ordered tokens\n",
    "- The larger n is, the larger the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>_data', 'data_science', 'science_is', 'is_fun', 'fun_<end>']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigram example:\n",
    "tokens = '<start> data science is fun <end>'.split()\n",
    "[tokens[i]+'_'+tokens[i+1] for i in range(len(tokens)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: TF and DF\n",
    "\n",
    "- **Term Frequency:** number of times a term is seen per document\n",
    "- $\\text{tf}(t, d) = \\text{count of term } t \\text{ in document } d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue', 'green', 'red']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['red green blue', 'red blue blue']\n",
    "\n",
    "#Vocabulary\n",
    "vocab = sorted(set(' '.join(corpus).split()))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      blue  green  red\n",
       "doc1   1.0    1.0  1.0\n",
       "doc2   2.0    0.0  1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF\n",
    "from collections import Counter\n",
    "tf = np.zeros((len(corpus),len(vocab)))\n",
    "for i,doc in enumerate(corpus):\n",
    "    for j,term in enumerate(vocab):\n",
    "        tf[i,j] = Counter(doc.split())[term]\n",
    "tf = pd.DataFrame(tf,index=['doc1','doc2'],columns=vocab)\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: TF and DF\n",
    "\n",
    "- **Document Frequency:** number of documents containing each term\n",
    "$\\text{df}(t) = \\text{count of documents containing term } t$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blue     2\n",
       "green    1\n",
       "red      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DF\n",
    "tf.astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Stopwords\n",
    "\n",
    "- terms that have high (or very low) DF and aren't informative\n",
    "  - common engish terms (ex: 'a', 'the','in',...)\n",
    "  - domain specific (ex, in class slides: 'data_science')\n",
    "  - often removed prior to analysis\n",
    "  - in sklearn\n",
    "    - `min_df`, an integer > 0, keep terms that occur in at at least n documents\n",
    "    - `max_df`, a float in (0,1], keep terms that occur in less than f% of total documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: CountVectorizer in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['blue green red', 'blue green green']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvect = CountVectorizer(lowercase=True,    # default, transform all docs to lowercase\n",
    "                        ngram_range=(1,1), # default, only unigrams\n",
    "                        min_df=1,          # default, keep all terms\n",
    "                        max_df=1.0,        # default, keep all terms\n",
    "                       )\n",
    "X_cv = cvect.fit_transform(corpus)\n",
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blue': 0, 'green': 1, 'red': 2}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect.vocabulary_ # learned vocabulary, term:index pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue', 'green', 'red']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect.get_feature_names() # vocabulary, sorted by indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1],\n",
       "        [1, 2, 0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cv.todense() # term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['blue', 'green', 'red'], dtype='<U5'),\n",
       " array(['blue', 'green'], dtype='<U5')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect.inverse_transform(X_cv) # mapping back to terms via vocabulary mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: TfIdf\n",
    "\n",
    "- What if some terms are still uninformative?\n",
    "- Can we downweight terms that occur in many documents?\n",
    "- **Term Frequency * Inverse Document Frequency (tf-idf)**\n",
    "  - $\\text{tf-idf}(t,d) = \\text{tf}(t, d) \\times \\text{idf}(t)$\n",
    "  - $\\text{idf}(t) = \\log \\frac{1+n}{1+\\text{df}(t)} + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue', 0), ('green', 1), ('red', 2)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvect = TfidfVectorizer(norm='l2') # by default, also doing l2 normalization\n",
    "\n",
    "X_tfidf = tfidfvect.fit_transform(corpus)\n",
    "sorted(tfidfvect.vocabulary_.items(),key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.50154891, 0.50154891, 0.70490949],\n",
       "        [0.4472136 , 0.89442719, 0.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 1., 1.],\n",
       "        [1., 2., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can also use to get term frequencies by setting use_idf to False and norm to none\n",
    "TfidfVectorizer(use_idf=False, norm=None).fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rec.sport.baseball\n",
      "--------------------------------------------------\n",
      "From: dougb@comm.mot.com (Doug Bank)\n",
      "Subject: Re: Info needed for Cleveland tickets\n",
      "Reply-To: dougb@ecs.comm.mot.com\n",
      "Organization: Motorola Land Mobile Products Sector\n",
      "Distribution: usa\n",
      "Nntp-Posting-Host: 145.1.146.35\n",
      "Lines: 17\n",
      "\n",
      "In article <1993Apr1.234031.4950@leland.Stanford.EDU>, bohnert@leland.Stanford.EDU (matthew bohnert) writes:\n",
      "\n",
      "|> I'm going to be in Cleveland Thursday, April 15 to Sunday, April 18.\n",
      "|> Does anybody know if the Tribe will be in town on those dates, and\n",
      "|> if so, who're they playing and if tickets are available?\n",
      "\n",
      "The tribe will be in town from April 16 to the 19th.\n",
      "There\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "ngs = fetch_20newsgroups(categories=['rec.sport.baseball','rec.sport.hockey']) # dataset has 20 categories, only get two\n",
    "\n",
    "docs_ngs = ngs['data']                         # get documents (emails)\n",
    "y_ngs = ngs['target']                          # get targets ([0,1])\n",
    "target_names_ngs = ngs['target_names']         # get target names (['rec.sport.baseball','rec.sport.hockey'])\n",
    "\n",
    "print(y_ngs[0], target_names_ngs[y_ngs[0]])  # print target int and target name\n",
    "print('-'*50)                                  # print a string of 50 dashes\n",
    "print(docs_ngs[0].strip()[:600])               # print beginning characters of first doc, after stripping whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Transform Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(897, 3760)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "docs_ngs_train,docs_ngs_test,y_ngs_train,y_ngs_test = train_test_split(docs_ngs,y_ngs)\n",
    "\n",
    "vect = TfidfVectorizer(lowercase=True,\n",
    "                       min_df=5,       # occur in at least 5 documents\n",
    "                       max_df=0.8,     # occur in at most 80% of documents\n",
    "                       token_pattern='\\\\b\\\\S\\\\S+\\\\b',  # tokens of at least 2 non-space characters\n",
    "                       ngram_range=(1,1),  # only unigrams\n",
    "                       use_idf=False,  # term frequency counts instead of tf-idf\n",
    "                       norm=None       # do not normalize\n",
    "                      )\n",
    "X_ngs_train = vect.fit_transform(docs_ngs_train)\n",
    "X_ngs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 1913),\n",
       " ('re', 2743),\n",
       " ('players', 2576),\n",
       " ('40', 176),\n",
       " ('college', 882)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first few terms in learned vocabulary\n",
    "list(vect.vocabulary_.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['design', 'saberhagen', '_americans_', 'shayne', 'coons']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first few terms in learned stopword list\n",
    "list(vect.stop_words_)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['king', 're', 'players', '40', 'college', 'ted', 'list', 'was',\n",
       "       'this', 'juan'], dtype='<U79')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first few terms in BOW representation of first document\n",
    "vect.inverse_transform(X_ngs_train[0])[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Train and Evaluate Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy cv accuracy: 0.51 +- 0.00\n",
      "lr    cv accuracy: 0.95 +- 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "scores_dummy = cross_val_score(DummyClassifier(strategy='most_frequent'),X_ngs_train,y_ngs_train)\n",
    "scores_lr = cross_val_score(LogisticRegression(),X_ngs_train,y_ngs_train)\n",
    "\n",
    "print(f'dummy cv accuracy: {scores_dummy.mean():0.2f} +- {scores_dummy.std():0.2f}')\n",
    "print(f'lr    cv accuracy: {scores_lr.mean():0.2f} +- {scores_lr.std():0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline accuracy on training set: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# use Pipeline instead of make_pipeline to add names to the steps\n",
    "# (name,object) tuple pairs for each step\n",
    "pipe_ngs = Pipeline([('vect',TfidfVectorizer(lowercase=True,\n",
    "                                             min_df=5,\n",
    "                                             max_df=0.8,\n",
    "                                             token_pattern='\\\\b\\\\S\\\\S+\\\\b',\n",
    "                                             ngram_range=(1,1),\n",
    "                                             use_idf=False,\n",
    "                                             norm=None )\n",
    "                     ),   \n",
    "                     ('lr',LogisticRegression())\n",
    "                    ])\n",
    "\n",
    "pipe_ngs.fit(docs_ngs_train,y_ngs_train) # pass in docs, not transformed X\n",
    "\n",
    "score_ngs = pipe_ngs.score(docs_ngs_train,y_ngs_train)\n",
    "print(f'pipeline accuracy on training set: {score_ngs:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipe cv accuracy: 0.95 +- 0.02\n"
     ]
    }
   ],
   "source": [
    "scores_pipe = cross_val_score(pipe_ngs,docs_ngs_train,y_ngs_train) \n",
    "print(f'pipe cv accuracy: {scores_pipe.mean():0.2f} +- {scores_pipe.std():0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 1913), ('re', 2743), ('players', 2576)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pipe_ngs['vect'].vocabulary_.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Add Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline accuracy on training set: 1.00\n",
      "pipe cv accuracy: 0.93 +- 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel,SelectPercentile\n",
    "\n",
    "pipe_ngs = Pipeline([('vect',TfidfVectorizer(lowercase=True,\n",
    "                                             min_df=5,\n",
    "                                             max_df=0.8,\n",
    "                                             token_pattern='\\\\b\\\\S\\\\S+\\\\b',\n",
    "                                             ngram_range=(1,1),\n",
    "                                             use_idf=False,\n",
    "                                             norm=None )\n",
    "                     ),   \n",
    "                     ('fs',SelectFromModel(estimator=LogisticRegression(C=1.0,\n",
    "                                                                        penalty='l1',\n",
    "                                                                        solver='liblinear',\n",
    "                                                                        max_iter=1000,\n",
    "                                                                        random_state=123\n",
    "                                                                       ))),\n",
    "                     ('lr',LogisticRegression(max_iter=1000))\n",
    "                    ])\n",
    "pipe_ngs.fit(docs_ngs_train,y_ngs_train)\n",
    "print(f'pipeline accuracy on training set: {pipe_ngs.score(docs_ngs_train,y_ngs_train):0.2f}')\n",
    "scores_pipe = cross_val_score(pipe_ngs,docs_ngs_train,y_ngs_train) \n",
    "print(f'pipe cv accuracy: {scores_pipe.mean():0.2f} +- {scores_pipe.std():0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Grid Search with Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gscsv best parameters  : {'fs__estimator__C': 1000, 'lr__C': 0.01, 'vect__ngram_range': (1, 1), 'vect__use_idf': True}\n",
      "gscsv best cv accuracy : 0.94\n",
      "gscsv test set accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this may take a minute or so\n",
    "params = {'vect__use_idf':[True,False],\n",
    "          'vect__ngram_range':[(1,1),(2,2)],\n",
    "          'fs__estimator__C':[10,1000],\n",
    "          'lr__C':[.01,1,100]}\n",
    "\n",
    "gscv = GridSearchCV(pipe_ngs, params, cv=2, n_jobs=-1).fit(docs_ngs_train,y_ngs_train)\n",
    "\n",
    "print(f'gscsv best parameters  : {gscv.best_params_}')\n",
    "print(f'gscsv best cv accuracy : {gscv.best_score_:0.2f}')\n",
    "print(f'gscsv test set accuracy: {gscv.score(docs_ngs_test,y_ngs_test):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentiment Analysis and sklearn\n",
    "<br>\n",
    "\n",
    "- determine sentiment/opinion from unstructured test\n",
    "- usually positive/negative, but is domain specific\n",
    "- can be treated as a classification task (with a target, using all of the tools we know)\n",
    "- can also be treated as a linguistic task (sentence parsing)\n",
    "<br>\n",
    "\n",
    "- Example: determine sentiment of movie reviews\n",
    "- see [sentiment_analysis_example.ipynb](sentiment_analysis_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling\n",
    "\n",
    "- What topics are our documents composed of?\n",
    "- How much of each topic does each document contain?\n",
    "- Can we represent documents using topic weights? (dimensionality reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is topic modeling?\n",
    "- How does Latent Dirichlet Allocation (LDA) work?\n",
    "- How to train and use LDA with sklearn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Topic Modeling?\n",
    "<br>\n",
    "\n",
    "- **topic:** a collection of related words\n",
    "- A document can be composed of several topics\n",
    "<br>\n",
    "\n",
    "- Given a collection of documents, we can ask:\n",
    "  - **What terms make up each topic?** (per topic term distribution)\n",
    "  - **What topics make up each document?** (per document topic distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- Unsupervised method for determining topics and topic assignments\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align=\"center\"><img src=\"images/lda_blei.jpg\" width=\"1100px\"></div>\n",
    "\n",
    "<font size=5>From David Blei</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two Important Matrices Learned by LDA\n",
    "\n",
    "- the **per topic term distributions** aka $\\varphi$ (phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>baseball</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic1</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat  baseball  play\n",
       "topic1  0.4       0.2   0.4\n",
       "topic2  0.2       0.4   0.4"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = ['topic1','topic2']\n",
    "vocab = ['cat','baseball','play']\n",
    "phi = pd.DataFrame([[0.4,.2,.4],[0.2,.4,.4]],columns=vocab,index=topics)\n",
    "phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the **per document term distributions** aka $\\theta$ (theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic1  topic2\n",
       "doc1     0.1     0.9\n",
       "doc2     0.5     0.5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = ['topic1','topic2']\n",
    "docs = ['doc1','doc2']\n",
    "theta = pd.DataFrame([[0.1,.9],[.5,.5]],columns=topics,index=docs)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling: Example\n",
    "\n",
    "- Given the data and the number of topics we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M = 3\n",
      "V = 6\n",
      "K = 2\n"
     ]
    }
   ],
   "source": [
    "corpus = ['the dog and cat played tennis',\n",
    "          'tennis and baseball are sports',\n",
    "          'a dog or a cat can be a pet']\n",
    "\n",
    "M = 3 # the number of documents\n",
    "\n",
    "vocab = ['baseball','cat','dog','pet','played','tennis']\n",
    "\n",
    "V = len(vocab) # size of vocabulary\n",
    "\n",
    "K = 2 # our guess about the number of topics\n",
    "\n",
    "print(f'{M = :}\\n{V = :}\\n{K = :}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling: Example\n",
    "\n",
    "- Guessing some **per topic term distributions** ($\\varphi$) given the documents and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baseball', 'cat', 'dog', 'pet', 'played', 'tennis']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseball</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>pet</th>\n",
       "      <th>played</th>\n",
       "      <th>tennis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic_1</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         baseball   cat   dog   pet  played  tennis\n",
       "topic_1      0.33  0.00  0.00  0.00    0.33    0.33\n",
       "topic_2      0.00  0.25  0.25  0.25    0.25    0.00"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the probability of each term given topic 1 (high for sports terms)\n",
    "topic_1 = [.33,   0,   0,   0, .33, .33]\n",
    "\n",
    "# the probability of each term given topic 2 (high for pet terms)\n",
    "topic_2 = [  0, .25, .25, .25, .25,   0]\n",
    "\n",
    "# per topic term distributions\n",
    "phi = pd.DataFrame([topic_1, topic_2],columns=vocab,\n",
    "                   index=['topic_'+str(x) for x in range(1,K+1)])\n",
    "\n",
    "phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling: Example\n",
    "\n",
    "- Guessing the **per document topic distributions** $\\theta$ given the **topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseball</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>pet</th>\n",
       "      <th>played</th>\n",
       "      <th>tennis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic_1</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         baseball   cat   dog   pet  played  tennis\n",
       "topic_1      0.33  0.00  0.00  0.00    0.33    0.33\n",
       "topic_2      0.00  0.25  0.25  0.25    0.25    0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['the dog and cat played tennis',\n",
       " 'tennis and baseball are sports',\n",
       " 'a dog or a cat can be a pet']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given our guess about phi\n",
    "display(phi)\n",
    "# And the corpus\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic_1  topic_2\n",
       "doc_1     0.50     0.50\n",
       "doc_2     0.99     0.01\n",
       "doc_3     0.01     0.99"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a guess about per document topic distributions\n",
    "theta = pd.DataFrame([[.50, .50],\n",
    "                      [.99, .01],\n",
    "                      [.01, .99]],\n",
    "                     columns=['topic_'+str(x) for x in range(1,K+1)],\n",
    "                     index=['doc_'+str(x) for x in range(1,M+1)])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling With LDA\n",
    "\n",
    "- Given\n",
    "  - a set of documents\n",
    "  - a number of topics $K$\n",
    "<br>\n",
    "\n",
    "- Learn\n",
    "  - the **per topic term distributions $\\varphi$ (phi)**, size: $K \\times V$\n",
    "  - the **per document topic distributions $\\theta$ (theta)**, size: $M \\times K$\n",
    "<br>\n",
    "\n",
    "- How to learn $\\varphi$ and $\\theta$:\n",
    "  - Latent Dirichlet Allocation (LDA)\n",
    "  - generative statistical model\n",
    "  - Blei, D., Ng, A., Jordan, M. Latent Dirichlet allocation. J. Mach. Learn. Res. 3 (Jan 2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling With LDA\n",
    "\n",
    "- Uses for $\\varphi$ (phi), the per topic term distributions:\n",
    "  - infering labels for topics\n",
    "  - word clouds\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Uses for $\\theta$ (theta), the per document topic distributions:\n",
    "  - dimentionality reduction\n",
    "  - clustering\n",
    "  - similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from all 20 newsgroups\n",
    "newsgroups = fetch_20newsgroups()\n",
    "ngs_all = newsgroups.data\n",
    "len(ngs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 4256)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform documents using tf-idf\n",
    "tfidf = TfidfVectorizer(token_pattern=r'\\b[a-zA-Z0-9-][a-zA-Z0-9-]+\\b',min_df=50, max_df=.2)\n",
    "X_tfidf = tfidf.fit_transform(ngs_all)\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '01', '02', '03', '04', '05', '06', '07', '08']\n",
      "['yours', 'yourself', 'ysu', 'zealand', 'zero', 'zeus', 'zip', 'zone', 'zoo', 'zuma']\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "print(feature_names[:10])\n",
    "print(feature_names[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA with sklearn Cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# create model with 20 topics\n",
    "lda = LatentDirichletAllocation(n_components=20,  # the number of topics\n",
    "                                n_jobs=-1,        # use all cpus\n",
    "                                random_state=123) # for reproducability\n",
    "\n",
    "# learn phi (lda.components_) and theta (X_lda)\n",
    "# this will take a while!\n",
    "X_lda = lda.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: tchen@magnus.acs.ohio-state.edu (Tsung-Kun Chen)\\nSubject: ** Software forsale (lots) **\\nNntp-P'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngs_all[100][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.01, 0.01, 0.01, 0.1 , 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "       0.01, 0.01, 0.01, 0.38, 0.01, 0.14, 0.01, 0.01, 0.28])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(X_lda[100],2) # lda representation of document_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 19, 16])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: since this is unsupervised, these numbers may change\n",
    "np.argsort(X_lda[100])[::-1][:3] # the top topics of document_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA: Per Topic Term Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# a utility function to print out the most likely terms for each topic\n",
    "# taken from https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic {:#2d}: \".format(topic_idx)\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0: uga ai georgia covington mcovingt\n",
      "Topic  1: digex access turkish armenian armenians\n",
      "Topic  2: god jesus bible christians christian\n",
      "Topic  3: values objective frank morality ap\n",
      "Topic  4: ohio-state magnus acs ohio cis\n",
      "Topic  5: caltech keith sandvik livesey sgi\n",
      "Topic  6: stratus msg usc indiana sw\n",
      "Topic  7: alaska uci aurora colostate nsmca\n",
      "Topic  8: wpi radar psu psuvm detector\n",
      "Topic  9: columbia utexas gatech cc prism\n",
      "Topic 10: scsi upenn simms ide bus\n",
      "Topic 11: nhl team mit players hockey\n",
      "Topic 12: lehigh duke jewish adobe ns1\n",
      "Topic 13: henry toronto zoo ti dseg\n",
      "Topic 14: sale card thanks please mac\n",
      "Topic 15: virginia joel hall doug douglas\n",
      "Topic 16: ca his new cs should\n",
      "Topic 17: cleveland cwru freenet cramer ins\n",
      "Topic 18: pitt gordon geb banks cs\n",
      "Topic 19: windows file window files thanks\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda,feature_names,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA Review\n",
    "\n",
    "- What did we learn?\n",
    "  - per document topic distributions\n",
    "  - per topic term distributions\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- What can we use this for?\n",
    "  - Dimensionality Reduction/Feature Extraction!\n",
    "  - investigate topics (much like PCA components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other NLP Features\n",
    "\n",
    "- Part of Speech tags\n",
    "- Dependency Parsing\n",
    "- Entity Detection\n",
    "- Word Vectors\n",
    "- See spaCy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using spaCy for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"N.Y.C.|is|n't|in|New|Jersey|.\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# uncomment the line below the first time you run this cell\n",
    "#%run -m spacy download en_core_web_sm\n",
    "try:\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "except OSError as e:\n",
    "    print('Need to run the following line in a new cell:')\n",
    "    print('%run -m spacy download en_core_web_sm')\n",
    "    print('or the following line from the commandline with eods-f20 activated:')\n",
    "    print('python -m spacy download en_core_web_sm')\n",
    "    \n",
    "parsed = nlp(\"N.Y.C. isn't in New Jersey.\")\n",
    "'|'.join([token.text for token in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text    lemma   pos   is_stop\n",
      "------------------------------\n",
      "Apple   Apple   PROPN False\n",
      "is      be      AUX   True\n",
      "looking look    VERB  False\n",
      "at      at      ADP   True\n",
      "buying  buy     VERB  False\n",
      "U.K.    U.K.    PROPN False\n",
      "startup startup NOUN  False\n",
      "for     for     ADP   True\n",
      "$       $       SYM   False\n",
      "1       1       NUM   False\n",
      "billion billion NUM   False\n",
      ".       .       PUNCT False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
    "\n",
    "print(f\"{'text':7s} {'lemma':7s} {'pos':5s} {'is_stop'}\")\n",
    "print('-'*30)\n",
    "for token in doc:\n",
    "    print(f'{token.text:7s} {token.lemma_:7s} {token.pos_:5s} {token.is_stop}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3eb261ff34c94a43be1d589a6e283f95-0\" class=\"displacy\" width=\"1975\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">billion.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3eb261ff34c94a43be1d589a6e283f95-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3eb261ff34c94a43be1d589a6e283f95-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3eb261ff34c94a43be1d589a6e283f95-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3eb261ff34c94a43be1d589a6e283f95-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3eb261ff34c94a43be1d589a6e283f95-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3eb261ff34c94a43be1d589a6e283f95-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3eb261ff34c94a43be1d589a6e283f95-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3eb261ff34c94a43be1d589a6e283f95-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3eb261ff34c94a43be1d589a6e283f95-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3eb261ff34c94a43be1d589a6e283f95-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3eb261ff34c94a43be1d589a6e283f95-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3eb261ff34c94a43be1d589a6e283f95-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3eb261ff34c94a43be1d589a6e283f95-0-6\" stroke-width=\"2px\" d=\"M770,264.5 C770,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3eb261ff34c94a43be1d589a6e283f95-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3eb261ff34c94a43be1d589a6e283f95-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3eb261ff34c94a43be1d589a6e283f95-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3eb261ff34c94a43be1d589a6e283f95-0-8\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3eb261ff34c94a43be1d589a6e283f95-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3eb261ff34c94a43be1d589a6e283f95-0-9\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,2.0 1800.0,2.0 1800.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3eb261ff34c94a43be1d589a6e283f95-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1800.0,266.5 L1808.0,254.5 1792.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Entity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ent.text,ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Word Vectors\n",
    "\n",
    "- word2vec\n",
    "- shallow neural net\n",
    "- predict a word given the surrounding context (SkipGram or CBOW)\n",
    "- words used in similar context should have similar vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Need either the _md or _lg models to get vector information\n",
    "# Note: this takes a while!\n",
    "#%run -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Baseball', (300,), [0.55838, 0.42791, -0.11687])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md') # _lg has a larger vocabulary\n",
    "\n",
    "doc = nlp('Baseball is played on a diamond.')\n",
    "doc[0].text, doc[0].vector.shape, list(doc[0].vector[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Multiple Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use nlp.pipe to transform multiple docs at once\n",
    "docs = list(nlp.pipe(['Baseball is played on a diamond.',\n",
    "                      'Hockey is played on ice.',\n",
    "                      'Diamonds are clear as ice.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1.00', '0.85', '0.76'],\n",
       "       ['0.85', '1.00', '0.77'],\n",
       "       ['0.76', '0.77', '1.00']], dtype='<U4')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using average of token vectors for each document.\n",
    "np.array([['{:.2f}'.format(docs[i].similarity(docs[j])) for j in range(3)]\n",
    "          for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning Sequences\n",
    "\n",
    "- Hidden Markov Models\n",
    "- Conditional Random Fields\n",
    "- Recurrant Neural Networks\n",
    "- LSTM\n",
    "- GPT3\n",
    "- [BERT](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Review\n",
    "\n",
    "- corpus, tokens, vocabulary, terms, n-grams, stopwords\n",
    "- tokenization\n",
    "- term frequency (TF), document frequency (DF)\n",
    "- TF vs TF-IDF\n",
    "- sentiment analysis\n",
    "- topic modeling\n",
    "<br>\n",
    "\n",
    "- POS\n",
    "- Dependency Parsing\n",
    "- Entity Extraction\n",
    "- Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# <center>Questions?</center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix: LDA Plate Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/Smoothed_LDA.png\" width=\"400px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<font size=5>\n",
    "    \n",
    "**K** :  number of topics\n",
    "\n",
    "**$\\varphi$** : per topic term distributions\n",
    "\n",
    "**$\\beta$**  : parameters for word distribution die factory, length = V (size of vocab)\n",
    "\n",
    "**M**     : number of documents\n",
    "\n",
    "**N**     : number of words/tokens in each document\n",
    "\n",
    "**$\\theta$** : per document topic distributions\n",
    "\n",
    "**$\\alpha$** : parameters for topic die factory, length = K (number of topics)\n",
    "\n",
    "**z** : topic indexes\n",
    "\n",
    "**w** : observed tokens\n",
    "\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "eods-f21",
   "language": "python",
   "name": "eods-f21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
